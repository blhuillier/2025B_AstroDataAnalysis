{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blhuillier/2025B_AstroDataAnalysis/blob/main/Notebooks/Chap_3_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzOjQURsUeq"
      },
      "source": [
        "# Exercises for Chapter 3 - Estimation\n",
        "Benjamin L'Huillier.\n",
        "Fall 2025\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz5mtYMoSwBc"
      },
      "source": [
        "# Exercise 1\n",
        "\n",
        "Let's apply the example:\n",
        "\n",
        "\n",
        "We consider the geometric distribution:\n",
        "$$\n",
        "f(x; p) = p(1-p)^{x-1}, \\quad x = 1, 2, 3, \\dots,\\quad p \\in (0,1)\n",
        "$$\n",
        "\n",
        "The expectation of the geometric distribution is:\n",
        "$$\n",
        "\\mathbb{E}[X] = \\frac{1}{p}\n",
        "$$\n",
        "(*You may try to prove this as a warm-up!*)\n",
        "\n",
        "1. Simulate Data.  \n",
        "a. Choose a value of $ p \\in (0,1) $ — for example, $ p = 0.3 $.  \n",
        "b. Generate $ N = 1000 $ samples from the geometric distribution using NumPy.  \n",
        "\n",
        "Recall that the log-likelihood is:\n",
        "$$\n",
        "\\log L(p) = \\sum_{i=1}^{N} \\log f(x_i; p)\n",
        "= N \\log p + \\left(\\sum_{i=1}^{N} (x_i - 1)\\right) \\log (1 - p)\n",
        "$$. \n",
        "2. Estimate $ \\hat{p} $ using Maximum Likelihood.  \n",
        "a.   Derive the maximum likelihood estimator $ \\hat{p}_{\\mathrm{MLE}} $.  \n",
        "b.  Then compute it from the data.  \n",
        "\n",
        "(*Hint:* You should get a simple expression involving the sample mean.)\n",
        "\n",
        "3. Compare Estimated vs True $ p $   \n",
        "a. Print both $ \\hat{p} $ and $ p_{\\mathrm{true}} $.  \n",
        "b. Optional: repeat the experiment for different values of $ p $, or different sample sizes $ N $.  \n",
        "c. Optional: plot a histogram of the data and compare it to the theoretical distribution with $ \\hat{p} $.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "# we fix the seed so that we obtain the same results everytime\n",
        "r = np.random.RandomState(seed=0)\n",
        "# use r to generate a sample. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise II — Pareto Distribution and Estimation  \n",
        "*Adapted from Feigelson & Babu, Exercise 4.7.1*\n",
        "\n",
        "The Pareto law is a truncated power law. It is used to describe self-similar processes — for example, the Salpeter initial mass function (IMF). Its probability density function (PDF) is:\n",
        "\n",
        "$$\n",
        "f(x) = \n",
        "\\begin{cases}\n",
        "\\frac{\\alpha b^\\alpha}{x^{\\alpha+1}} & \\text{if } x \\geq b \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "where $\\alpha > 0$, $b > 0$.\n",
        "\n",
        "1. Compute the cumulative distribution function (CDF) $F(x)$.  \n",
        "\n",
        "2. Let $U_i \\sim \\mathcal{U}(0,1)$ for $i = 1, \\dots, 500$. Use the inverse CDF method to construct $X_i = \\phi(U_i)$ such that $X_i \\sim \\mathrm{Pareto}(b=1, \\alpha=1.35)$.  \n",
        "\n",
        "It can be shown that the MLE estimators for a sample $\\{X_1, \\dots, X_N\\}$ from a Pareto distribution are:\n",
        "\n",
        "$$\n",
        "\\hat{b} = \\min_i X_i,\\quad\n",
        "\\hat{\\alpha} = \\frac{N}{\\sum_i \\ln \\left( \\frac{X_i}{b} \\right)}\n",
        "$$\n",
        "\n",
        "3. Compute $\\hat{b}$ and $\\hat{\\alpha}$ for your generated data.\n",
        "\n",
        "MLEs for the Pareto distribution are biased. A bias-corrected version is:\n",
        "\n",
        "$$\n",
        "\\alpha^* = \\left(1 - \\frac{2}{N} \\right)\\hat{\\alpha},\\quad\n",
        "b^* = \\left(1 - \\frac{1}{(N - 1)\\hat{\\alpha}} \\right)\\hat{b}\n",
        "$$\n",
        "\n",
        "4. Calculate $\\alpha^*$ and $b^*$ using your sample.  \n",
        "\n",
        "5. Method of Moments:  \n",
        "   a. Show that for $\\alpha > 1$, the expected value is:  \n",
        "   $$\n",
        "   \\mathbb{E}[X] = \\frac{\\alpha b}{\\alpha - 1}\n",
        "   $$\n",
        "   Note: For $\\alpha < 2$, the variance is undefined.  \n",
        "\n",
        "   b. Assume $b = 1$. Express $\\alpha$ as a function of the sample mean.  \n",
        "\n",
        "   c. Estimate $\\alpha$ using the method of moments for your sample.\n",
        "\n",
        "6. Compare the estimators $\\hat{\\alpha}$, $\\alpha^*$, and the moment-based estimate in terms of:  \n",
        "   - Bias  \n",
        "   - Variance  \n",
        "\n",
        "   You may repeat the experiment multiple times to compute empirical bias and variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iix-2cz-ycXv"
      },
      "source": [
        "# Exercise III — Confidence Intervals for the Mean\n",
        "\n",
        "Let us verify the confidence interval for the mean of a normally distributed variable:\n",
        "\n",
        "$$\n",
        "X \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
        "$$\n",
        "\n",
        "1. Generate one realization of $(X_1, \\dots, X_N) \\sim \\mathcal{N}(\\mu, \\sigma^2)$ for values of $\\mu$ and $\\sigma$ of your choice.\n",
        "\n",
        "2. Assume that $\\sigma^2$ is known. Estimate the 95% confidence interval for $\\mu$ using the standard normal distribution.\n",
        "\n",
        "3. Repeat the experiment $N_{\\mathrm{exp}} = 1000$ times. In how many cases does the true $\\mu$ fall within the confidence interval?\n",
        "\n",
        "4. Now assume that $\\sigma^2$ is unknown.  \n",
        "   - Compute the sample mean $\\bar{X}$ and the (unbiased) sample variance $S_N$.  \n",
        "   - Estimate the 95% confidence interval for $\\mu$, using the $t$-distribution with $N - 1$ degrees of freedom.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
